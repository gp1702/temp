{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import HIVTreatment_Gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards):\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegressor:\n",
    "    def __init__(self, D):\n",
    "        self.w = np.random.randn(D) / np.sqrt(D)\n",
    "        self.lr = 0.1\n",
    "    def partial_fit(self, X, Y):\n",
    "        self.w += self.lr*(Y - X.dot(self.w)).dot(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, env):\n",
    "        # observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "        # NOTE!! state samples are poor, b/c you get velocities --> infinity\n",
    "        observation_examples = np.random.random((20000, 6))*2 - 1\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(observation_examples)\n",
    "\n",
    "        # Used to converte a state to a featurizes represenation.\n",
    "        # We use RBF kernels with different variances to cover different parts of the space\n",
    "        featurizer = FeatureUnion([\n",
    "                (\"rbf1\", RBFSampler(gamma=0.05, n_components=1000)),\n",
    "                (\"rbf2\", RBFSampler(gamma=1.0, n_components=1000)),\n",
    "                (\"rbf3\", RBFSampler(gamma=0.5, n_components=1000)),\n",
    "                (\"rbf4\", RBFSampler(gamma=0.1, n_components=1000))\n",
    "                ])\n",
    "        feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "        self.dimensions = feature_examples.shape[1]\n",
    "        self.scaler = scaler\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "    def transform(self, observations):\n",
    "        scaled = self.scaler.transform(observations)\n",
    "        return self.featurizer.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds one SGDRegressor for each action\n",
    "class Model:\n",
    "    def __init__(self, env, feature_transformer):\n",
    "        self.env = env\n",
    "        self.models = []\n",
    "        self.feature_transformer = feature_transformer\n",
    "        for i in range(env.action_space.n):\n",
    "            model = SGDRegressor(feature_transformer.dimensions)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, s):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        result = np.stack([m.predict(X) for m in self.models]).T\n",
    "        return result\n",
    "    \n",
    "    def update(self, s, a, G):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        self.models[a].partial_fit(X, [G])\n",
    "\n",
    "    def sample_action(self, s, eps):\n",
    "        features=self.predict\n",
    "        if np.random.random() < eps:\n",
    "              return self.env.action_space.sample()\n",
    "        else:\n",
    "              return np.argmax(self.predict(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, eps, gamma):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    action = model.sample_action(observation, eps)\n",
    "    for iters in range(2000):\n",
    "        obs_features=model.predict(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "#         if done:\n",
    "#             reward = -200\n",
    "\n",
    "    \n",
    "        next_obs_features = model.predict(next_observation)\n",
    "        \n",
    "        next_action=model.sample_action(next_observation,eps)\n",
    "        \n",
    "        \n",
    "        # print(next.shape)\n",
    "        assert(next_obs_features.shape == (1, env.action_space.n))\n",
    "        assert(obs_features.shape)==(1,env.action_space.n)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        G = reward + gamma*(eps*np.mean(next_obs_features)+(1-eps)*np.max(next_obs_features))\n",
    "       \n",
    "        model.update(observation, action, G)\n",
    "        totalreward += reward\n",
    "#         if reward == 1: # if we changed the reward to -200\n",
    "#             totalreward += reward\n",
    "        iters += 1\n",
    "        observation=next_observation\n",
    "        action=next_action\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('HIV-v0')\n",
    "    ft = FeatureTransformer(env)\n",
    "    model = Model(env, ft)\n",
    "    gamma = 0.99\n",
    "\n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "\n",
    "\n",
    "    N = 10000\n",
    "    totalrewards = np.empty(N)\n",
    "    costs = np.empty(N)\n",
    "    for n in range(N):\n",
    "        eps = 1.0/np.sqrt(n+1)\n",
    "        totalreward = play_one(env, model, eps, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_running_avg(totalrewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach :\n",
    "  The previous implementations of KBRL are unable to converge thus we now move to a different choice of kernel and method of implementation. We decide to use the Radial Basis Function (RBF) kernel.\n",
    "  \n",
    "  The RBF kernel model can be thought of as a linear  model with feature extraction where the feature extractor is a RBF kernel.\n",
    "  \n",
    "  Mathematically the RBF kernel can be expressed as:\n",
    "  $$\\phi (x)=exp(-\\frac{||x-c||^{2}}{\\sigma^{2}})$$\n",
    "  \n",
    "  where:\n",
    "         $x$= input vector\n",
    "         $c$ = center or exemplar vector\n",
    "         $\\sigma$ = scale parameter.\n",
    "         \n",
    "  Note that $c$ is a hyper-parameter and needs to be found by experimentation.\n",
    "  \n",
    "  We use the RBF-sampler from scikit-learn and sample from the state space to train a linear regresor to learns its feature.\n",
    "  Since we are not sure which scale parameter $\\sigma$ is good we use multiple RBF kernels with different scale parameters. We also use one step of a gradient descent to integrate it with the Q learning mechanism.\n",
    "  \n",
    "Another implementation detail worth noting is that we only extract the features from the state $s$ and learn a different linear-model for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
