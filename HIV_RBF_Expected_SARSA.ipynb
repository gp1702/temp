{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import HIVTreatment_Gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards):\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegressor:\n",
    "    def __init__(self, D):\n",
    "        self.w = np.random.randn(D) / np.sqrt(D)\n",
    "        self.lr = 0.1\n",
    "    def partial_fit(self, X, Y):\n",
    "        self.w += self.lr*(Y - X.dot(self.w)).dot(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, env):\n",
    "        # observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "        # NOTE!! state samples are poor, b/c you get velocities --> infinity\n",
    "        observation_examples = np.random.random((20000, 6))*2 - 1\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(observation_examples)\n",
    "\n",
    "        # Used to converte a state to a featurizes represenation.\n",
    "        # We use RBF kernels with different variances to cover different parts of the space\n",
    "        featurizer = FeatureUnion([\n",
    "                (\"rbf1\", RBFSampler(gamma=0.05, n_components=1000)),\n",
    "                (\"rbf2\", RBFSampler(gamma=1.0, n_components=1000)),\n",
    "                (\"rbf3\", RBFSampler(gamma=0.5, n_components=1000)),\n",
    "                (\"rbf4\", RBFSampler(gamma=0.1, n_components=1000))\n",
    "                ])\n",
    "        feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "        self.dimensions = feature_examples.shape[1]\n",
    "        self.scaler = scaler\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "    def transform(self, observations):\n",
    "        scaled = self.scaler.transform(observations)\n",
    "        return self.featurizer.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds one SGDRegressor for each action\n",
    "class Model:\n",
    "    def __init__(self, env, feature_transformer):\n",
    "        self.env = env\n",
    "        self.models = []\n",
    "        self.feature_transformer = feature_transformer\n",
    "        for i in range(env.action_space.n):\n",
    "            model = SGDRegressor(feature_transformer.dimensions)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, s):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        result = np.stack([m.predict(X) for m in self.models]).T\n",
    "        return result\n",
    "    \n",
    "    def update(self, s, a, G):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        self.models[a].partial_fit(X, [G])\n",
    "\n",
    "    def sample_action(self, s, eps):\n",
    "        features=self.predict\n",
    "        if np.random.random() < eps:\n",
    "              return self.env.action_space.sample()\n",
    "        else:\n",
    "              return np.argmax(self.predict(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, eps, gamma):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    action = model.sample_action(observation, eps)\n",
    "    for iters in range(2000):\n",
    "        obs_features=model.predict(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "#         if done:\n",
    "#             reward = -200\n",
    "\n",
    "    \n",
    "        next_obs_features = model.predict(next_observation)\n",
    "        \n",
    "        next_action=model.sample_action(next_observation,eps)\n",
    "        \n",
    "        \n",
    "        # print(next.shape)\n",
    "        assert(next_obs_features.shape == (1, env.action_space.n))\n",
    "        assert(obs_features.shape)==(1,env.action_space.n)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        G = reward + gamma*(eps*np.mean(next_obs_features)+(1-eps)*np.max(next_obs_features))\n",
    "       \n",
    "        model.update(observation, action, G)\n",
    "        totalreward += reward\n",
    "#         if reward == 1: # if we changed the reward to -200\n",
    "#             totalreward += reward\n",
    "        iters += 1\n",
    "        observation=next_observation\n",
    "        action=next_action\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('HIV-v0')\n",
    "    ft = FeatureTransformer(env)\n",
    "    model = Model(env, ft)\n",
    "    gamma = 0.99\n",
    "\n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "\n",
    "\n",
    "    N = 10000\n",
    "    totalrewards = np.empty(N)\n",
    "    costs = np.empty(N)\n",
    "    for n in range(N):\n",
    "        eps = 1.0/np.sqrt(n+1)\n",
    "        totalreward = play_one(env, model, eps, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_running_avg(totalrewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 8269699.0058665145 eps: 1.0 avg reward (last 100): 8269699.0058665145\n",
      "episode: 100 total reward: 86777213.65309471 eps: 0.09950371902099892 avg reward (last 100): 61533099.48731169\n",
      "episode: 200 total reward: 363670460.64014554 eps: 0.07053456158585983 avg reward (last 100): 194130578.22342452\n",
      "episode: 300 total reward: 1539597856.4161475 eps: 0.0576390417704235 avg reward (last 100): 946241001.2526941\n",
      "episode: 400 total reward: 2955519113.3726735 eps: 0.04993761694389223 avg reward (last 100): 2076703215.3491054\n",
      "episode: 500 total reward: 3349316709.3589344 eps: 0.04467670516087703 avg reward (last 100): 2871581377.0918756\n",
      "episode: 600 total reward: 4598426821.676602 eps: 0.04079085082240021 avg reward (last 100): 3668868031.5166125\n",
      "episode: 700 total reward: 21169003099.899784 eps: 0.0377694787300249 avg reward (last 100): 11416573640.173096\n",
      "episode: 800 total reward: 15995745070.105684 eps: 0.03533326266687867 avg reward (last 100): 20149850564.913788\n",
      "episode: 900 total reward: 22338393858.356007 eps: 0.03331483023263848 avg reward (last 100): 21653164972.975792\n",
      "episode: 1000 total reward: 27466488196.63141 eps: 0.0316069770620507 avg reward (last 100): 23408353652.343647\n",
      "episode: 1100 total reward: 29117120907.382843 eps: 0.03013743873394561 avg reward (last 100): 24467125142.639957\n",
      "episode: 1200 total reward: 25968974858.774677 eps: 0.028855492841238062 avg reward (last 100): 24821118723.696976\n",
      "episode: 1300 total reward: 21189378063.2767 eps: 0.027724348650071385 avg reward (last 100): 25769071976.628117\n",
      "episode: 1400 total reward: 29724695225.5237 eps: 0.02671658425726324 avg reward (last 100): 27542836660.439865\n",
      "episode: 1500 total reward: 26733287941.545063 eps: 0.025811286645983367 avg reward (last 100): 27791260678.635162\n",
      "episode: 1600 total reward: 23640070164.920197 eps: 0.02499219116020307 avg reward (last 100): 30145879977.503296\n",
      "episode: 1700 total reward: 27906287818.17607 eps: 0.024246432248443597 avg reward (last 100): 29659631380.069492\n",
      "episode: 1800 total reward: 32315420526.626938 eps: 0.02356368148131365 avg reward (last 100): 31456319104.1992\n",
      "episode: 1900 total reward: 36733465833.469376 eps: 0.02293553851298437 avg reward (last 100): 32123849746.96354\n",
      "episode: 2000 total reward: 42119790078.6096 eps: 0.022355091700494795 avg reward (last 100): 32979385766.583332\n",
      "episode: 2100 total reward: 23212499758.378838 eps: 0.021816595214404266 avg reward (last 100): 32819316432.16831\n",
      "episode: 2200 total reward: 41645386667.94424 eps: 0.021315227815974374 avg reward (last 100): 34190164486.626686\n",
      "episode: 2300 total reward: 32322936329.192097 eps: 0.02084690996125416 avg reward (last 100): 33190015686.156246\n",
      "episode: 2400 total reward: 41655135273.654854 eps: 0.02040816326530612 avg reward (last 100): 33825061572.897537\n",
      "episode: 2500 total reward: 35018253523.60172 eps: 0.01999600119960014 avg reward (last 100): 34450669695.80759\n",
      "episode: 2600 total reward: 28286269264.37101 eps: 0.0196078431372549 avg reward (last 100): 34188669777.726955\n",
      "episode: 2700 total reward: 37979898059.45426 eps: 0.019241446072101123 avg reward (last 100): 34166873911.57981\n",
      "episode: 2800 total reward: 36827374317.95331 eps: 0.018894849871330582 avg reward (last 100): 34353032509.124825\n",
      "episode: 2900 total reward: 43464937281.418045 eps: 0.018566333001716968 avg reward (last 100): 35682915700.76259\n",
      "episode: 3000 total reward: 41049251318.98951 eps: 0.01825437644092281 avg reward (last 100): 34707322694.90384\n",
      "episode: 3100 total reward: 26050336314.715305 eps: 0.017957634043632188 avg reward (last 100): 34969655926.445274\n",
      "episode: 3200 total reward: 34618137514.53636 eps: 0.01767490804100673 avg reward (last 100): 35132208453.026146\n",
      "episode: 3300 total reward: 36013488068.885635 eps: 0.01740512865461766 avg reward (last 100): 34566486534.779106\n",
      "episode: 3400 total reward: 35777502781.3361 eps: 0.017147337032429676 avg reward (last 100): 35940090017.08868\n",
      "episode: 3500 total reward: 38685374582.095345 eps: 0.01690067088544646 avg reward (last 100): 36195810336.7726\n",
      "episode: 3600 total reward: 41510935888.55948 eps: 0.016664352333993333 avg reward (last 100): 35342282192.153076\n",
      "episode: 3700 total reward: 36159184235.9802 eps: 0.016437677572823703 avg reward (last 100): 35162836568.23433\n",
      "episode: 3800 total reward: 37314275154.911026 eps: 0.01622000804188198 avg reward (last 100): 36408663920.37595\n",
      "episode: 3900 total reward: 37367135637.652534 eps: 0.01601076285016887 avg reward (last 100): 35605389253.46789\n",
      "episode: 4000 total reward: 35230101394.72924 eps: 0.015809412247806517 avg reward (last 100): 36619419584.646675\n",
      "episode: 4100 total reward: 32078605134.85883 eps: 0.015615471979112765 avg reward (last 100): 35312847011.2335\n",
      "episode: 4200 total reward: 31980414970.083416 eps: 0.015428498379527544 avg reward (last 100): 35527949376.816\n",
      "episode: 4300 total reward: 38448788802.494804 eps: 0.015248084103296531 avg reward (last 100): 36397889475.22681\n",
      "episode: 4400 total reward: 34173335742.36 eps: 0.015073854388204487 avg reward (last 100): 36777116209.917366\n",
      "episode: 4500 total reward: 35819175122.34191 eps: 0.014905463779355262 avg reward (last 100): 37066526793.67622\n",
      "episode: 4600 total reward: 32629645619.074127 eps: 0.014742593246782542 avg reward (last 100): 36435474105.26748\n",
      "episode: 4700 total reward: 34996312771.02016 eps: 0.014584947642137372 avg reward (last 100): 36661681546.449165\n",
      "episode: 4800 total reward: 29130232155.629402 eps: 0.014432253448298278 avg reward (last 100): 37386591697.72921\n",
      "episode: 4900 total reward: 43964113966.31507 eps: 0.014284256782850143 avg reward (last 100): 38219723612.27287\n",
      "episode: 5000 total reward: 43871216207.88181 eps: 0.014140721622265264 avg reward (last 100): 37433800272.96072\n",
      "episode: 5100 total reward: 42137652570.03503 eps: 0.014001428218521149 avg reward (last 100): 37669015505.1905\n",
      "episode: 5200 total reward: 37355028759.01725 eps: 0.013866171683985996 avg reward (last 100): 37230450852.12878\n",
      "episode: 5300 total reward: 38019898195.755516 eps: 0.013734760723838819 avg reward (last 100): 37278509112.214745\n",
      "episode: 5400 total reward: 38429151075.18139 eps: 0.013607016498184134 avg reward (last 100): 37883599204.80954\n",
      "episode: 5500 total reward: 37812099609.48598 eps: 0.013482771598464758 avg reward (last 100): 36886892621.55198\n",
      "episode: 5600 total reward: 43652564946.323685 eps: 0.01336186912484746 avg reward (last 100): 37684719306.98455\n",
      "episode: 5700 total reward: 41315416815.85461 eps: 0.013244161853017133 avg reward (last 100): 36861375666.987434\n",
      "episode: 5800 total reward: 36619309495.76461 eps: 0.013129511480316922 avg reward (last 100): 37492292556.78603\n",
      "episode: 5900 total reward: 41699499613.02902 eps: 0.013017787942456283 avg reward (last 100): 37213247308.54921\n",
      "episode: 6000 total reward: 42794589300.61208 eps: 0.012908868793110689 avg reward (last 100): 38466840063.055565\n",
      "episode: 6100 total reward: 33253161348.451305 eps: 0.012802638639684321 avg reward (last 100): 37237794422.7975\n",
      "episode: 6200 total reward: 34348996061.681767 eps: 0.012698988629324323 avg reward (last 100): 38094813471.105515\n",
      "episode: 6300 total reward: 36780772612.019966 eps: 0.012597815979981676 avg reward (last 100): 37956169335.21653\n",
      "episode: 6400 total reward: 41725723761.51315 eps: 0.01249902355192602 avg reward (last 100): 36789454895.765335\n",
      "episode: 6500 total reward: 41004533160.44103 eps: 0.01240251945565374 avg reward (last 100): 37840421975.86997\n",
      "episode: 6600 total reward: 33365909862.15116 eps: 0.01230821669259179 avg reward (last 100): 38362708974.331276\n",
      "episode: 6700 total reward: 36433117019.61353 eps: 0.012216032825403858 avg reward (last 100): 37506009674.581825\n",
      "episode: 6800 total reward: 31872275505.4594 eps: 0.01212588967505907 avg reward (last 100): 38011891458.571236\n",
      "episode: 6900 total reward: 35653324139.32647 eps: 0.0120377130421331 avg reward (last 100): 37577975221.471725\n",
      "episode: 7000 total reward: 43063057019.776535 eps: 0.011951432450083671 avg reward (last 100): 38423703738.88808\n",
      "episode: 7100 total reward: 36052708467.74763 eps: 0.011866980908481739 avg reward (last 100): 37392463613.8762\n",
      "episode: 7200 total reward: 37984443495.711426 eps: 0.01178429469439066 avg reward (last 100): 37282810215.59308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7300 total reward: 42994181927.26673 eps: 0.011703313150272007 avg reward (last 100): 38539757417.20395\n",
      "episode: 7400 total reward: 38137243664.89807 eps: 0.011623978496961555 avg reward (last 100): 38141330659.29909\n",
      "episode: 7500 total reward: 43504695349.16264 eps: 0.01154623566040508 avg reward (last 100): 38314091152.673004\n",
      "episode: 7600 total reward: 41403830932.03655 eps: 0.011470032110973345 avg reward (last 100): 38702579186.01313\n",
      "episode: 7700 total reward: 29708817780.165436 eps: 0.011395317714291038 avg reward (last 100): 38874613065.72657\n",
      "episode: 7800 total reward: 29306413179.508057 eps: 0.011322044592617122 avg reward (last 100): 37692595072.28461\n",
      "episode: 7900 total reward: 41702682514.79606 eps: 0.011250166995905777 avg reward (last 100): 38433514915.04463\n",
      "episode: 8000 total reward: 33269794678.076683 eps: 0.01117964118175896 avg reward (last 100): 38450869759.24588\n",
      "episode: 8100 total reward: 43465032039.81436 eps: 0.011110425303554916 avg reward (last 100): 38426392061.78837\n",
      "episode: 8200 total reward: 39419800125.4152 eps: 0.0110424793061026 avg reward (last 100): 38613734278.32391\n",
      "episode: 8300 total reward: 38406342014.97002 eps: 0.010975764828230913 avg reward (last 100): 38463470274.491554\n",
      "episode: 8400 total reward: 37217498170.137375 eps: 0.010910245111774533 avg reward (last 100): 38751127179.933044\n",
      "episode: 8500 total reward: 42927275343.935905 eps: 0.01084588491646583 avg reward (last 100): 38124721116.99949\n",
      "episode: 8600 total reward: 36733844721.43289 eps: 0.010782650440285157 avg reward (last 100): 38375492869.01191\n",
      "episode: 8700 total reward: 30155428054.780083 eps: 0.010720509244860595 avg reward (last 100): 38292833715.94246\n",
      "episode: 8800 total reward: 43363500147.513 eps: 0.010659430185543134 avg reward (last 100): 38569269752.7058\n",
      "episode: 8900 total reward: 43329380990.26671 eps: 0.01059938334581489 avg reward (last 100): 39331053622.76524\n",
      "episode: 9000 total reward: 40213875096.77836 eps: 0.010540339975716555 avg reward (last 100): 38593997567.07195\n",
      "episode: 9100 total reward: 43799054200.46399 eps: 0.010482272434006255 avg reward (last 100): 39104068619.304665\n",
      "episode: 9200 total reward: 43138183786.99678 eps: 0.010425154133785426 avg reward (last 100): 38413655211.57786\n",
      "episode: 9300 total reward: 43467326822.51167 eps: 0.01036895949134883 avg reward (last 100): 38615645861.8634\n",
      "episode: 9400 total reward: 42234526888.52294 eps: 0.010313663878035112 avg reward (last 100): 38806977744.16063\n",
      "episode: 9500 total reward: 34273588430.739685 eps: 0.010259243574872122 avg reward (last 100): 39029421516.08269\n",
      "episode: 9600 total reward: 37902915100.39694 eps: 0.010205675729827259 avg reward (last 100): 38455174777.79217\n",
      "episode: 9700 total reward: 42901254740.45125 eps: 0.010152938317487875 avg reward (last 100): 38240081935.42615\n",
      "episode: 9800 total reward: 42074028612.98447 eps: 0.010101010101010102 avg reward (last 100): 39377039797.219894\n",
      "episode: 9900 total reward: 43545598060.63028 eps: 0.010049870596186849 avg reward (last 100): 38852366489.76715\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach :\n",
    "  The previous implementations of KBRL are unable to converge thus we now move to a different choice of kernel and method of implementation. We decide to use the Radial Basis Function (RBF) kernel.\n",
    "  \n",
    "  The RBF kernel model can be thought of as a linear  model with feature extraction where the feature extractor is a RBF kernel.\n",
    "  \n",
    "  Mathematically the RBF kernel can be expressed as:\n",
    "  $$\\phi (x)=exp(-\\frac{||x-c||^{2}}{\\sigma^{2}})$$\n",
    "  \n",
    "  where:\n",
    "         $x$= input vector\n",
    "         $c$ = center or exemplar vector\n",
    "         $\\sigma$ = scale parameter.\n",
    "         \n",
    "  Note that $c$ is a hyper-parameter and needs to be found by experimentation.\n",
    "  \n",
    "  We use the RBF-sampler from scikit-learn and sample from the state space to train a linear regresor to learns its feature.\n",
    "  Since we are not sure which scale parameter $\\sigma$ is good we use multiple RBF kernels with different scale parameters. We also use one step of a gradient descent to integrate it with the Q learning mechanism.\n",
    "  \n",
    "Another implementation detail worth noting is that we only extract the features from the state $s$ and learn a different linear-model for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
