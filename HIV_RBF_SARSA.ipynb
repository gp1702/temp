{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import HIVTreatment_Gym\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_avg(totalrewards):\n",
    "    N = len(totalrewards)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "    plt.plot(running_avg)\n",
    "    plt.title(\"Running Average\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegressor:\n",
    "    def __init__(self, D):\n",
    "        self.w = np.random.randn(D) / np.sqrt(D)\n",
    "        self.lr = 0.1\n",
    "    def partial_fit(self, X, Y):\n",
    "        self.w += self.lr*(Y - X.dot(self.w)).dot(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self, env):\n",
    "        # observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "        # NOTE!! state samples are poor, b/c you get velocities --> infinity\n",
    "        observation_examples = np.random.random((20000, 6))*2 - 1\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(observation_examples)\n",
    "\n",
    "        # Used to converte a state to a featurizes represenation.\n",
    "        # We use RBF kernels with different variances to cover different parts of the space\n",
    "        featurizer = FeatureUnion([\n",
    "                (\"rbf1\", RBFSampler(gamma=0.05, n_components=1000)),\n",
    "                (\"rbf2\", RBFSampler(gamma=1.0, n_components=1000)),\n",
    "                (\"rbf3\", RBFSampler(gamma=0.5, n_components=1000)),\n",
    "                (\"rbf4\", RBFSampler(gamma=0.1, n_components=1000))\n",
    "                ])\n",
    "        feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "        self.dimensions = feature_examples.shape[1]\n",
    "        self.scaler = scaler\n",
    "        self.featurizer = featurizer\n",
    "\n",
    "    def transform(self, observations):\n",
    "        scaled = self.scaler.transform(observations)\n",
    "        return self.featurizer.transform(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holds one SGDRegressor for each action\n",
    "class Model:\n",
    "    def __init__(self, env, feature_transformer):\n",
    "        self.env = env\n",
    "        self.models = []\n",
    "        self.feature_transformer = feature_transformer\n",
    "        for i in range(env.action_space.n):\n",
    "            model = SGDRegressor(feature_transformer.dimensions)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, s):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        result = np.stack([m.predict(X) for m in self.models]).T\n",
    "        return result\n",
    "    \n",
    "    def update(self, s, a, G):\n",
    "        X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "        self.models[a].partial_fit(X, [G])\n",
    "\n",
    "    def sample_action(self, s, eps):\n",
    "        if np.random.random() < eps:\n",
    "              return self.env.action_space.sample()\n",
    "        else:\n",
    "              return np.argmax(self.predict(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, eps, gamma):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    action = model.sample_action(observation, eps)\n",
    "    for iters in range(2000):\n",
    "        obs_features=model.predict(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "\n",
    "#         if done:\n",
    "#             reward = -200\n",
    "\n",
    "    \n",
    "        next_obs_features = model.predict(next_observation)\n",
    "        \n",
    "        next_action=model.sample_action(next_observation,eps)\n",
    "        \n",
    "        \n",
    "        # print(next.shape)\n",
    "        assert(next_obs_features.shape == (1, env.action_space.n))\n",
    "        assert(obs_features.shape)==(1,env.action_space.n)\n",
    "        #import pdb;pdb.set_trace()\n",
    "        G = reward + gamma*(next_obs_features[0,next_action])\n",
    "       \n",
    "        model.update(observation, action, G)\n",
    "        totalreward += reward\n",
    "#         if reward == 1: # if we changed the reward to -200\n",
    "#             totalreward += reward\n",
    "        iters += 1\n",
    "        observation=next_observation\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('HIV-v0')\n",
    "    ft = FeatureTransformer(env)\n",
    "    model = Model(env, ft)\n",
    "    gamma = 0.99\n",
    "\n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "\n",
    "\n",
    "    N = 10000\n",
    "    totalrewards = np.empty(N)\n",
    "    costs = np.empty(N)\n",
    "    for n in range(N):\n",
    "        eps = 1.0/np.sqrt(n+1)\n",
    "        totalreward = play_one(env, model, eps, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_running_avg(totalrewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 3432351.2660789876 eps: 1.0 avg reward (last 100): 3432351.2660789876\n",
      "episode: 100 total reward: 3192408.2608373947 eps: 0.09950371902099892 avg reward (last 100): 4852735.642571799\n",
      "episode: 200 total reward: 7021980.933276117 eps: 0.07053456158585983 avg reward (last 100): 4695326.245361185\n",
      "episode: 300 total reward: 3978357.2789319986 eps: 0.0576390417704235 avg reward (last 100): 4453517.930315732\n",
      "episode: 400 total reward: 3978357.2789319986 eps: 0.04993761694389223 avg reward (last 100): 4697572.450007374\n",
      "episode: 500 total reward: 3432351.2660789876 eps: 0.04467670516087703 avg reward (last 100): 4195436.706437932\n",
      "episode: 600 total reward: 3978357.2789319986 eps: 0.04079085082240021 avg reward (last 100): 4252562.6830004575\n",
      "episode: 700 total reward: 7021980.933276117 eps: 0.0377694787300249 avg reward (last 100): 4836705.674375466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b4a6262fa8ed>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtotalreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtotalrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-a283caadbba4>\u001b[0m in \u001b[0;36mplay_one\u001b[0;34m(env, model, eps, gamma)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mobs_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         if done:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/McGill/Comp 767 Reinforcement Learning/Project/temp/HIVTreatment_Gym.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0meps1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         ns = odeint(dsdt, self.state, [0, self.dt],\n\u001b[0;32m---> 98\u001b[0;31m                     args=(eps1, eps2), mxstep=1000)[-1]\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# the reward function penalizes treatment because of side-effects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/McGill/thesis/lib/python3.6/site-packages/scipy/integrate/odepack.py\u001b[0m in \u001b[0;36modeint\u001b[0;34m(func, y0, t, args, Dfun, col_deriv, full_output, ml, mu, rtol, atol, tcrit, h0, hmax, hmin, ixpr, mxstep, mxhnil, mxordn, mxords, printmessg)\u001b[0m\n\u001b[1;32m    213\u001b[0m     output = _odepack.odeint(func, y0, t, args, Dfun, col_deriv, ml, mu,\n\u001b[1;32m    214\u001b[0m                              \u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcrit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                              ixpr, mxstep, mxhnil, mxordn, mxords)\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mwarning_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_msgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Run with full_output = 1 to get quantitative information.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/McGill/Comp 767 Reinforcement Learning/Project/temp/HIVTreatment_Gym.py\u001b[0m in \u001b[0;36mdsdt\u001b[0;34m(s, t, eps1, eps2)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0mdT2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT2s\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT2s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0mdV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meps2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT1s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mT2s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m          - ((1. - eps1) * rho1 * k1 * T1 +\n\u001b[0m\u001b[1;32m    209\u001b[0m             (1. - f * eps1) * rho2 * k2 * T2) * V\n\u001b[1;32m    210\u001b[0m     \u001b[0mdE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambdaE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT1s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mT2s\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT1s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mT2s\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mE\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach :\n",
    "  The previous implementations of KBRL are unable to converge thus we now move to a different choice of kernel and method of implementation. We decide to use the Radial Basis Function (RBF) kernel.\n",
    "  \n",
    "  The RBF kernel model can be thought of as a linear  model with feature extraction where the feature extractor is a RBF kernel.\n",
    "  \n",
    "  Mathematically the RBF kernel can be expressed as:\n",
    "  $$\\phi (x)=exp(-\\frac{||x-c||^{2}}{\\sigma^{2}})$$\n",
    "  \n",
    "  where:\n",
    "         $x$= input vector\n",
    "         $c$ = center or exemplar vector\n",
    "         $\\sigma$ = scale parameter.\n",
    "         \n",
    "  Note that $c$ is a hyper-parameter and needs to be found by experimentation.\n",
    "  \n",
    "  We use the RBF-sampler from scikit-learn and sample from the state space to train a linear regresor to learns its feature.\n",
    "  Since we are not sure which scale parameter $\\sigma$ is good we use multiple RBF kernels with different scale parameters. We also use one step of a gradient descent to integrate it with the Q learning mechanism.\n",
    "  \n",
    "Another implementation detail worth noting is that we only extract the features from the state $s$ and learn a different linear-model for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
